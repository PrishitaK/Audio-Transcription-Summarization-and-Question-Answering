{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_6kSkZ_hC_I"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wget\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_directory = 'PATH_TO_YOUR_FOLDER'"
      ],
      "metadata": {
        "id": "pVp4xf2d5xUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pAX8xLyihE7"
      },
      "outputs": [],
      "source": [
        "def download_podcasts(soup, title):\n",
        "  i = 0\n",
        "  time_all = [] #Dates of realeases of the podcast episodes\n",
        "  ID_all = [] #ID of the podcast episodes\n",
        "  description_all = [] #Description of each podcast episode\n",
        "  length_all = [] #Duration of each podcast episode\n",
        "  link_all = [] #Link to each podcast episode\n",
        "  name_all = [] #Names of the podcast episodes\n",
        "  for podcast in soup.find_all('a', {'role': 'listitem'}):\n",
        "    time = podcast.find('div', {'class': 'OTz6ee'}).text\n",
        "    #Downloading all episodes released in December 2022\n",
        "    if (time.startswith('Dec')) and (time.endswith('2022')):\n",
        "      i += 1\n",
        "      time_all.append(time) #Date of release of the episode\n",
        "\n",
        "      link = podcast.find('div', {'jsname': 'fvi9Ef'})['jsdata'].split(';')[1]\n",
        "      link_all.append(link) #Link for the episode\n",
        "\n",
        "      name = podcast.find('div', {'class': 'e3ZUqe'}).text\n",
        "      name_all.append(name) #Name of the episode\n",
        "\n",
        "      print(i, ':', time)\n",
        "      filename = wget.download(link, out=path_to_directory+title) #Downloading the episode audio file\n",
        "      os.rename(filename, path_to_directory+title+'/audio'+str(i)+'.mp3')\n",
        "      ID_all.append('audio'+str(i)) #ID for the episode\n",
        "\n",
        "      #Looking to see if the description of the episode is present\n",
        "      try:\n",
        "        description = podcast.find('div', {'class': 'LrApYe'}).text\n",
        "      except:\n",
        "        description = 'None'\n",
        "\n",
        "      if description is None:\n",
        "        description = 'None'\n",
        "      description_all.append(description)\n",
        "\n",
        "      length = podcast.find('span', {'class': 'gUJ0Wc'}).text\n",
        "      length_all.append(length) #Duration of the episode\n",
        "\n",
        "  #Saving all the information relating to the podcast episode as a pandas dataframe\n",
        "  df = pd.DataFrame(list(zip([title]*len(ID_all), ID_all, name_all,\n",
        "                             time_all,\n",
        "                             description_all,\n",
        "                             length_all,\n",
        "                             link_all)),\n",
        "                    columns = ['Show', 'ID', 'Episode', 'Time',\n",
        "                               'Description', 'Length', 'Link'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqi-gKZ9hUEe"
      },
      "outputs": [],
      "source": [
        "#URLs of the podcasts: 'The Stack Overflow Podcast' and 'Myths and Legends'\n",
        "URLs = ['LINKS_TO_PODCASTS']\n",
        "\n",
        "info_df_list = []\n",
        "\n",
        "for url in URLs:\n",
        "  #Using BeautifulSoup to scrape and parse HTML content from the web pages of the two podcasts using lxml parser\n",
        "  soup = BeautifulSoup(requests.get(url).text, 'lxml')\n",
        "  title = soup.find('div', {'class': 'ZfMIwb'}).text #To retrieve the title of the podcast\n",
        "  os.mkdir(path_to_directory+title)\n",
        "  df = download_podcasts(soup, title) #Calling the previously defined function to download the relevant episodes and return their data\n",
        "  info_df_list.append(df)\n",
        "\n",
        "info_all_podcasts = pd.concat(info_df_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azj6csmYrkZg"
      },
      "outputs": [],
      "source": [
        "info_all_podcasts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the dataframe as a csv file for future use\n",
        "info_all_podcasts.to_csv(path_to_directory+'podcast.csv')"
      ],
      "metadata": {
        "id": "41XbvNQyhLMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}